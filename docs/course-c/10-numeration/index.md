# Numération

La [[numération]] regroupe les conventions par lesquelles nous représentons les nombres — qu'ils soient cardinaux ou ordinaux —, les bases qui structurent ces écritures (par exemple les systèmes binaire, ternaire, quinaire, décimal ou vicésimal) ainsi que les méthodes de codification comme IEEE 754, le [[complément à un]] ou le [[complément à deux]]. Maîtriser ces notions constitue un prérequis indispensable pour toute personne amenée à manipuler des données numériques, car de nombreuses opérations de bas niveau reposent sur ces fondements.

Ce chapitre s'adresse d'abord aux personnes qui écrivent du code proche du matériel, aux électronicien·nes et aux informaticien·nes techniques, mais la compréhension des représentations numériques bénéficie en réalité à tout esprit curieux. Saisir comment un ordinateur encode un nombre et, plus largement, une information, favorise la rédaction de programmes efficaces et la conception d'algorithmes robustes. En définitive, l'étude de la numération offre un accès privilégié à la manière dont la machine manipule les données, en partant de l'unité la plus élémentaire : le [[bit|bit|bit, le]].
